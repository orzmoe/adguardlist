package main

import (
	"bufio"
	"bytes"
	"fmt"
	"io"
	"log"
	"net/http"
	"os"
	"os/exec"
	"path/filepath"
	"strings"
	"sync"
	"time"
)

const (
	rulesFile         = "setting/rules.txt"
	outputDir         = "rules"
	publishDir        = "publish"
	outputFile        = "output.txt"
	tempMergedFile    = "merged_rules.txt"
	tempCompiledFile  = "compiled_rules.txt"
	maxConcurrentJobs = 8
	downloadTimeout   = 45 * time.Second
)

// downloadResult ä¿å­˜äº†ä¸‹è½½ä»»åŠ¡çš„å†…å®¹å’Œå¯èƒ½å‘ç”Ÿçš„é”™è¯¯ã€‚
type downloadResult struct {
	url     string
	content []byte
	err     error
}

// readLines å°†æ•´ä¸ªæ–‡ä»¶è¯»å…¥å†…å­˜ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²åˆ‡ç‰‡ã€‚
func readLines(path string) ([]string, error) {
	file, err := os.Open(path)
	if err != nil {
		return nil, err
	}
	defer file.Close()

	var lines []string
	scanner := bufio.NewScanner(file)
	for scanner.Scan() {
		line := strings.TrimSpace(scanner.Text())
		if line != "" && !strings.HasPrefix(line, "#") {
			lines = append(lines, line)
		}
	}
	return lines, scanner.Err()
}

// downloadWorker æ˜¯ä¸€ä¸ªå·¥ä½œåç¨‹ï¼Œå®ƒä» jobs é€šé“æ¥æ”¶ URLï¼Œ
// ä¸‹è½½åå°†ç»“æœå‘é€åˆ° results é€šé“ã€‚
func downloadWorker(id int, jobs <-chan string, results chan<- downloadResult, wg *sync.WaitGroup) {
	defer wg.Done()
	client := &http.Client{
		Timeout: downloadTimeout,
	}
	for url := range jobs {
		log.Printf("[Worker %d] Downloading %s\n", id, url)
		var result downloadResult
		result.url = url

		req, err := http.NewRequest("GET", url, nil)
		if err != nil {
			result.err = fmt.Errorf("failed to create request: %w", err)
			results <- result
			continue
		}
		req.Header.Set("User-Agent", "Mozilla/5.0 (compatible; AdRulesBot-Go/1.0)")

		resp, err := client.Do(req)
		if err != nil {
			result.err = fmt.Errorf("http request failed: %w", err)
			results <- result
			continue
		}

		if resp.StatusCode != http.StatusOK {
			result.err = fmt.Errorf("bad status: %s", resp.Status)
			results <- result
			resp.Body.Close()
			continue
		}

		body, err := io.ReadAll(resp.Body)
		resp.Body.Close()
		if err != nil {
			result.err = fmt.Errorf("failed to read body: %w", err)
			results <- result
			continue
		}

		if len(body) == 0 {
			result.err = fmt.Errorf("downloaded file is empty")
			results <- result
			continue
		}

		result.content = body
		results <- result
	}
}

// countRules è®¡ç®—æ–‡ä»¶ä¸­çš„æœ‰æ•ˆè§„åˆ™æ•°é‡ï¼Œè·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œã€‚
func countRules(content []byte) int {
	scanner := bufio.NewScanner(bytes.NewReader(content))
	count := 0
	for scanner.Scan() {
		line := strings.TrimSpace(scanner.Text())
		if line != "" && !strings.HasPrefix(line, "!") && !strings.HasPrefix(line, "#") {
			count++
		}
	}
	return count
}

func main() {
	log.Println("ğŸš€ Starting AdGuard rules processing with Go...")

	// 1. ä»è§„åˆ™æ–‡ä»¶ä¸­è¯»å– URL
	urls, err := readLines(rulesFile)
	if err != nil {
		log.Fatalf("âŒ Failed to read rules file '%s': %v", rulesFile, err)
	}
	totalSources := len(urls)
	log.Printf("â„¹ï¸ Found %d rule sources in '%s'.", totalSources, rulesFile)

	// 2. å¹¶å‘ä¸‹è½½æ‰€æœ‰è§„åˆ™
	jobs := make(chan string, totalSources)
	results := make(chan downloadResult, totalSources)
	var wg sync.WaitGroup

	for i := 1; i <= maxConcurrentJobs; i++ {
		wg.Add(1)
		go downloadWorker(i, jobs, results, &wg)
	}

	for _, url := range urls {
		jobs <- url
	}
	close(jobs)

	var successfulDownloads [][]byte
	var failedDownloads []string
	for i := 0; i < totalSources; i++ {
		res := <-results
		if res.err != nil {
			log.Printf("âŒ Download failed for %s: %v", res.url, res.err)
			failedDownloads = append(failedDownloads, res.url)
		} else {
			log.Printf("âœ… Downloaded %s (%d bytes)", res.url, len(res.content))
			successfulDownloads = append(successfulDownloads, res.content)
		}
	}
	wg.Wait() // ç­‰å¾…æ‰€æœ‰ worker å®Œæˆ

	successCount := len(successfulDownloads)
	failedCount := len(failedDownloads)
	log.Printf("ğŸ“Š Download summary: %d successful, %d failed.", successCount, failedCount)

	if successCount == 0 {
		log.Fatal("âŒ No rules were downloaded successfully. Aborting.")
	}

	// 3. åˆå¹¶å·²ä¸‹è½½çš„è§„åˆ™
	log.Println("ğŸ”„ Merging downloaded rules...")
	mergedContent := bytes.Join(successfulDownloads, []byte("\n"))
	if err := os.WriteFile(tempMergedFile, mergedContent, 0644); err != nil {
		log.Fatalf("âŒ Failed to write merged rules to '%s': %v", tempMergedFile, err)
	}
	defer os.Remove(tempMergedFile)

	// 4. è¿è¡Œ hostlist-compiler
	log.Println("âš™ï¸ Compiling rules with hostlist-compiler...")
	cmd := exec.Command("hostlist-compiler", "-i", tempMergedFile, "-o", tempCompiledFile)
	cmd.Stdout = os.Stdout
	cmd.Stderr = os.Stderr
	if err := cmd.Run(); err != nil {
		log.Fatalf("âŒ hostlist-compiler failed: %v", err)
	}
	defer os.Remove(tempCompiledFile)

	compiledContent, err := os.ReadFile(tempCompiledFile)
	if err != nil {
		log.Fatalf("âŒ Failed to read compiled file '%s': %v", tempCompiledFile, err)
	}

	// 5. ç”Ÿæˆæœ€ç»ˆçš„è¾“å‡ºæ–‡ä»¶
	log.Println("ğŸ“ Generating final output file...")
	ruleCount := countRules(compiledContent)
	buildTime := time.Now().Format(time.RFC3339)

	var header bytes.Buffer
	header.WriteString("# Title: 5whys Adguard Home Rules List (Use with a lot of false rejects)\n")
	header.WriteString(fmt.Sprintf("# Version: %s\n", time.Now().Format("200601021504")))
	header.WriteString(fmt.Sprintf("# Generated: %s\n", buildTime))
	header.WriteString("# Expires: 12 hours\n")
	header.WriteString(fmt.Sprintf("# Total sources: %d (Success: %d, Failed: %d)\n", totalSources, successCount, failedCount))
	header.WriteString(fmt.Sprintf("# Total rules: %d\n", ruleCount))
	header.WriteString(fmt.Sprintf("# Homepage: https://github.com/%s\n", os.Getenv("GITHUB_REPOSITORY")))
	header.WriteString("#\n")
	header.WriteString("# Source URLs:\n")
	for _, url := range urls {
		header.WriteString(fmt.Sprintf("# - %s\n", url))
	}
	header.WriteString("#\n")
	header.WriteString("####################################################################################\n\n")

	finalContent := append(header.Bytes(), compiledContent...)

	// 6. åˆ›å»ºç›®å½•å¹¶å†™å…¥æ–‡ä»¶
	if err := os.MkdirAll(outputDir, 0755); err != nil {
		log.Fatalf("âŒ Failed to create output directory '%s': %v", outputDir, err)
	}
	if err := os.MkdirAll(publishDir, 0755); err != nil {
		log.Fatalf("âŒ Failed to create publish directory '%s': %v", publishDir, err)
	}

	outputFilePath := filepath.Join(outputDir, outputFile)
	publishFilePath := filepath.Join(publishDir, outputFile)

	if err := os.WriteFile(outputFilePath, finalContent, 0644); err != nil {
		log.Fatalf("âŒ Failed to write final output to '%s': %v", outputFilePath, err)
	}
	log.Printf("âœ… Wrote output to %s", outputFilePath)

	// æ‹·è´åˆ° publish ç›®å½•
	if err := os.WriteFile(publishFilePath, finalContent, 0644); err != nil {
		log.Fatalf("âŒ Failed to copy output to '%s': %v", publishFilePath, err)
	}
	log.Printf("âœ… Copied output to %s", publishFilePath)

	// ä¸ºåç»­æ­¥éª¤è®¾ç½® GITHUB_ENV
	githubEnvFile := os.Getenv("GITHUB_ENV")
	if githubEnvFile != "" {
		f, err := os.OpenFile(githubEnvFile, os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644)
		if err != nil {
			log.Printf("âš ï¸ Could not open GITHUB_ENV file: %v", err)
		} else {
			defer f.Close()
			envVars := map[string]int{
				"RULES_COUNT":   ruleCount,
				"SUCCESS_COUNT": successCount,
				"FAILED_COUNT":  failedCount,
				"TOTAL_COUNT":   totalSources,
			}
			for key, val := range envVars {
				if _, err := f.WriteString(fmt.Sprintf("%s=%d\n", key, val)); err != nil {
					log.Printf("âš ï¸ Failed to write %s to GITHUB_ENV: %v", key, err)
				}
			}
		}
	}

	log.Println("âœ… All tasks completed successfully.")
}
